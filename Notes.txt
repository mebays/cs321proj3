For the project went thru and gathered all links 

IDEAS
1. go through and while retreiving the urls write to csv file possibly?
	a. If write the csv file then go through and retreive all unique urls and write those files, The ones that are traversed, use the net 
		use the net location as the file name to link the url with the file name. 

	b. If can successfully do that should be fairly easy to create a mapping

	c. Figure out how to traverse only 5 webpages deep from the home page if not then file will be running for awhile

2. When working through page acquire html pages and the plain text write html to file with link name, followed by after adding link to csv write words only 
	a. This would create multiple csv files one for everypage but allow pandas to access multiple tables to join them to find the most used link

	b. Have until 3 tomorrow to figure out most of these by then have to start running the program to allow time to run over night if need be.
Objective 2 

creating the csv files go through and create 

if option 2 in ideas the csv files would already be created to acquire the most used links on every page since duplicates would be allowed. 


Remind Chris to write the paper for proj3. 
